{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem statement:** \n",
    "\n",
    "Continuous bag of words (cbow) word2vec word embedding work is that it tends to predict the probability of a word given a context. A context may be a single word or a group of words. But for simplicity, I will take a single context word and try to predict a single target word. \n",
    "\n",
    "The purpose of this assignment is to be able to create a word embedding for the  given data set.  \n",
    "\n",
    "**Data set :** w2v.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten,Embedding,Dense\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the maximum length of an array required to include all sentences. Vocabulary size, in order to create the \n",
    "# same size inputs by padding with zeros\n",
    "\n",
    "def get_max_vocabulary_size(encoded_line_words):\n",
    "    max_voc_size = 0\n",
    "    max_integer_index = 0\n",
    "    for l in encoded_line_words:\n",
    "        if (len(l) > 0) and (np.argmax(l) > max_integer_index):\n",
    "            max_integer_index = l[np.argmax(l)]\n",
    "        if max_voc_size < len(l):\n",
    "            max_voc_size = len(l)\n",
    "    return max_voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new list with the targets for the words with window size as window_size. \n",
    "# Only positive window size is taken into consideration\n",
    "\n",
    "def map_target(context, window_size):\n",
    "    if window_size < 0:\n",
    "        print(\"Only Positive window sie is expected.\")\n",
    "        return;\n",
    "    target = context[window_size:]\n",
    "    m = 10\n",
    "    i = 0\n",
    "    while i < window_size :\n",
    "        target = np.append(target, [0])\n",
    "        i = i + 1\n",
    "    return np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating array of targets using the map_target function\n",
    "\n",
    "def map_all_targets(context_list, window_size):\n",
    "    targets = []\n",
    "    for context in context_list:\n",
    "        target_row = map_target(context, window_size)\n",
    "        targets.append(np.array(target_row))\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences_words(file_lines_words, maxlen):\n",
    "    padded_word = []\n",
    "    for line_word in file_lines_words:\n",
    "        blank_word_index = len(line_word)\n",
    "        while blank_word_index < maxlen:\n",
    "            line_word.append(\"\")\n",
    "            padded_word.append(line_word)\n",
    "            blank_word_index = blank_word_index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def corpus_to_vocab(txt_file):\n",
    "    corpus = ''\n",
    "    with open(txt_file, 'r', encoding=\"utf-8\") as file:\n",
    "        try:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    line = line.strip()\n",
    "                    if line!=\"\":\n",
    "                        corpus = corpus + ' ' + line\n",
    "                except Exception as E:\n",
    "                    print (\"got An exception 2: \", E)\n",
    "                    pass         \n",
    "        except Exception as E:\n",
    "            print (\"got An exception 3: \", E)\n",
    "            pass\n",
    "        corpus = re.sub('[\\.\\,\\\"\\'\\(\\)\\n\\s]+', ' ', corpus.strip().lower())\n",
    "        return corpus.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_words_list(word_list):\n",
    "    unique_words = []\n",
    "    for w in word_list:\n",
    "        if (w not in unique_words) and len(w)>2:\n",
    "            unique_words.append(w)\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_OHE(vocabulary):\n",
    "    OHE_word = [one_hot(wd.strip(),len(vocabulary))[0] for wd in vocabulary]\n",
    "    OHE_word_pair = {wd.strip():one_hot(wd.strip(),len(vocabulary))[0] for wd in vocabulary}\n",
    "    Word_OHE_pair = {one_hot(wd.strip(),len(vocabulary))[0]:wd.strip() for wd in vocabulary}\n",
    "    return {'Word':vocabulary, 'OHE': OHE_word, 'Word_OHE':OHE_word_pair, 'OHE_Word': Word_OHE_pair}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_list = corpus_to_vocab(\"w2v.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = unique_words_list(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = word_OHE(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [target for target in dictionary['OHE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary['OHE'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "targets = []\n",
    "for target in dictionary['OHE']:\n",
    "    if index < len(dictionary['OHE'])-1:\n",
    "        targets.append(dictionary['OHE'][index+1])\n",
    "    index = index + 1\n",
    "targets.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = np.array(dictionary['OHE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the parameters calculation and constants are being set using this function. \n",
    "# Any changes to the parameters should be mafe here to avoid execution errors.\n",
    "\n",
    "def parameters(context):\n",
    "    reshaped_context = context.reshape(-1)\n",
    "    INPUT_DIM = max(reshaped_context)+1\n",
    "    OUTPUT_DIM = 8\n",
    "    INPUT_LENGTH = len(context)\n",
    "    EPOCHS = 1000\n",
    "    VERBOSE = 1\n",
    "    LOSS = 'categorical_crossentropy'\n",
    "    ACTIVATION = 'softmax'\n",
    "    OPTIMIZER = 'adam'\n",
    "    MATRIX = ['accuracy']\n",
    "    BESTMODEL = 'embeddings.h5'\n",
    "    return (INPUT_DIM, OUTPUT_DIM, INPUT_LENGTH, EPOCHS, VERBOSE, LOSS, ACTIVATION, OPTIMIZER, MATRIX, BESTMODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 72, 153,  28, 110, 212, 107, 224, 145, 190, 100,  42, 181, 197,\n",
       "       174, 194, 129,  79,  13, 192, 242, 185, 140,  39, 241,  93, 203,\n",
       "       234,  13, 105, 102, 104, 222,  50, 207, 210, 196,   6, 160,  50,\n",
       "       173, 228,  56, 196,  69, 246,  75,  49, 137, 152,  78, 173, 101,\n",
       "       169,  31,  10,   2, 108,  62,  84,  53, 115,  64,  41,  42, 132,\n",
       "       226, 176, 232, 240,  87, 129, 126, 221, 156, 172, 114, 207, 236,\n",
       "         2, 179, 220,  55, 254, 172, 238,  69, 197, 127, 186,  62, 160,\n",
       "       163, 166, 116, 149,  75,  20,  85, 215, 167,  18, 171, 243,  35,\n",
       "        87, 252, 141, 170, 253, 254,  83, 191, 142, 246, 136, 240, 153,\n",
       "       127, 189, 171,  34, 106,  45,  92,  39, 171, 116, 250, 157, 208,\n",
       "        23,  71, 152, 146, 122,  94, 108,  55, 136, 100,  93, 232,  92,\n",
       "        11,  73, 152,  40, 129, 159,  34, 190, 164, 253,  99,  61,  84,\n",
       "       191,  84, 210, 129, 101, 227,  79, 239, 204,   1, 210, 193, 129,\n",
       "       165,  45,   4,  20, 118, 215,  84, 118, 118, 202, 247, 200, 100,\n",
       "       222, 211, 228,  45, 107,  94,  73, 179, 158, 247, 188, 244,  96,\n",
       "       244,  30, 252, 133,   4, 114, 190, 164,  12,  67, 226,  82, 176,\n",
       "        90,  57, 149, 108, 245, 173,  37,  51, 129,  32, 209, 250,   9,\n",
       "        21, 164,  18, 227,  37, 205,  46, 140, 136, 110, 167,  81, 100,\n",
       "       156, 195,   7, 254, 119,   7,  94, 125,   7, 134, 201,  12, 152,\n",
       "       236,  92,  38, 120,  79, 109, 136, 148])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((255,), (255,))"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirming that the shapes of context and targets are same\n",
    "\n",
    "context.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = context.reshape(-1, 255)\n",
    "targets = targets.reshape(-1, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 255), (255,))"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 72, 153,  28, 110, 212, 107, 224, 145, 190, 100,  42, 181, 197,\n",
       "        174, 194, 129,  79,  13, 192, 242, 185, 140,  39, 241,  93, 203,\n",
       "        234,  13, 105, 102, 104, 222,  50, 207, 210, 196,   6, 160,  50,\n",
       "        173, 228,  56, 196,  69, 246,  75,  49, 137, 152,  78, 173, 101,\n",
       "        169,  31,  10,   2, 108,  62,  84,  53, 115,  64,  41,  42, 132,\n",
       "        226, 176, 232, 240,  87, 129, 126, 221, 156, 172, 114, 207, 236,\n",
       "          2, 179, 220,  55, 254, 172, 238,  69, 197, 127, 186,  62, 160,\n",
       "        163, 166, 116, 149,  75,  20,  85, 215, 167,  18, 171, 243,  35,\n",
       "         87, 252, 141, 170, 253, 254,  83, 191, 142, 246, 136, 240, 153,\n",
       "        127, 189, 171,  34, 106,  45,  92,  39, 171, 116, 250, 157, 208,\n",
       "         23,  71, 152, 146, 122,  94, 108,  55, 136, 100,  93, 232,  92,\n",
       "         11,  73, 152,  40, 129, 159,  34, 190, 164, 253,  99,  61,  84,\n",
       "        191,  84, 210, 129, 101, 227,  79, 239, 204,   1, 210, 193, 129,\n",
       "        165,  45,   4,  20, 118, 215,  84, 118, 118, 202, 247, 200, 100,\n",
       "        222, 211, 228,  45, 107,  94,  73, 179, 158, 247, 188, 244,  96,\n",
       "        244,  30, 252, 133,   4, 114, 190, 164,  12,  67, 226,  82, 176,\n",
       "         90,  57, 149, 108, 245, 173,  37,  51, 129,  32, 209, 250,   9,\n",
       "         21, 164,  18, 227,  37, 205,  46, 140, 136, 110, 167,  81, 100,\n",
       "        156, 195,   7, 254, 119,   7,  94, 125,   7, 134, 201,  12, 152,\n",
       "        236,  92,  38, 120,  79, 109, 136, 148]),\n",
       " 153)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unit testing the context and it's relavent target. Change the index between 0 to 12 to check the pair\n",
    "\n",
    "index = 0\n",
    "context[index], targets[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM, OUTPUT_DIM, INPUT_LENGTH, EPOCHS, VERBOSE, LOSS, ACTIVATION, OPTIMIZER, MATRIX, BESTMODEL = \\\n",
    "parameters(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np\\nimport tensorflow as tf'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calback array to save the best model. Best model is decided on the basis of accuracy.\n",
    "\n",
    "callback_list = [tf.keras.callbacks.ModelCheckpoint(filepath=BESTMODEL, monitor = MATRIX[0], \n",
    "                                                            save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the sequential model with Embedding layer with output dimention of 8\n",
    "# The weights of the first layers will be used as the word embedding\n",
    "# these weights will later be used to calculate the distance between the words.\n",
    "# least distant words are most related and vice versa\n",
    "\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(input_dim=INPUT_DIM, output_dim=OUTPUT_DIM, input_length=255)\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "\n",
    "# A softmax activation is used.\n",
    "\n",
    "model.add(Dense(255, activation = ACTIVATION))\n",
    "\n",
    "model.compile(optimizer = OPTIMIZER, loss=LOSS, metrics = MATRIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 255, 8)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 2040)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 255)               520455    \n",
      "=================================================================\n",
      "Total params: 522,495\n",
      "Trainable params: 522,495\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Summary of the model \n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 72, 153,  28, 110, 212, 107, 224, 145, 190, 100,  42, 181, 197,\n",
       "        174, 194, 129,  79,  13, 192, 242, 185, 140,  39, 241,  93, 203,\n",
       "        234,  13, 105, 102, 104, 222,  50, 207, 210, 196,   6, 160,  50,\n",
       "        173, 228,  56, 196,  69, 246,  75,  49, 137, 152,  78, 173, 101,\n",
       "        169,  31,  10,   2, 108,  62,  84,  53, 115,  64,  41,  42, 132,\n",
       "        226, 176, 232, 240,  87, 129, 126, 221, 156, 172, 114, 207, 236,\n",
       "          2, 179, 220,  55, 254, 172, 238,  69, 197, 127, 186,  62, 160,\n",
       "        163, 166, 116, 149,  75,  20,  85, 215, 167,  18, 171, 243,  35,\n",
       "         87, 252, 141, 170, 253, 254,  83, 191, 142, 246, 136, 240, 153,\n",
       "        127, 189, 171,  34, 106,  45,  92,  39, 171, 116, 250, 157, 208,\n",
       "         23,  71, 152, 146, 122,  94, 108,  55, 136, 100,  93, 232,  92,\n",
       "         11,  73, 152,  40, 129, 159,  34, 190, 164, 253,  99,  61,  84,\n",
       "        191,  84, 210, 129, 101, 227,  79, 239, 204,   1, 210, 193, 129,\n",
       "        165,  45,   4,  20, 118, 215,  84, 118, 118, 202, 247, 200, 100,\n",
       "        222, 211, 228,  45, 107,  94,  73, 179, 158, 247, 188, 244,  96,\n",
       "        244,  30, 252, 133,   4, 114, 190, 164,  12,  67, 226,  82, 176,\n",
       "         90,  57, 149, 108, 245, 173,  37,  51, 129,  32, 209, 250,   9,\n",
       "         21, 164,  18, 227,  37, 205,  46, 140, 136, 110, 167,  81, 100,\n",
       "        156, 195,   7, 254, 119,   7,  94, 125,   7, 134, 201,  12, 152,\n",
       "        236,  92,  38, 120,  79, 109, 136, 148]])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([153,  28, 110, 212, 107, 224, 145, 190, 100,  42, 181, 197, 174,\n",
       "       194, 129,  79,  13, 192, 242, 185, 140,  39, 241,  93, 203, 234,\n",
       "        13, 105, 102, 104, 222,  50, 207, 210, 196,   6, 160,  50, 173,\n",
       "       228,  56, 196,  69, 246,  75,  49, 137, 152,  78, 173, 101, 169,\n",
       "        31,  10,   2, 108,  62,  84,  53, 115,  64,  41,  42, 132, 226,\n",
       "       176, 232, 240,  87, 129, 126, 221, 156, 172, 114, 207, 236,   2,\n",
       "       179, 220,  55, 254, 172, 238,  69, 197, 127, 186,  62, 160, 163,\n",
       "       166, 116, 149,  75,  20,  85, 215, 167,  18, 171, 243,  35,  87,\n",
       "       252, 141, 170, 253, 254,  83, 191, 142, 246, 136, 240, 153, 127,\n",
       "       189, 171,  34, 106,  45,  92,  39, 171, 116, 250, 157, 208,  23,\n",
       "        71, 152, 146, 122,  94, 108,  55, 136, 100,  93, 232,  92,  11,\n",
       "        73, 152,  40, 129, 159,  34, 190, 164, 253,  99,  61,  84, 191,\n",
       "        84, 210, 129, 101, 227,  79, 239, 204,   1, 210, 193, 129, 165,\n",
       "        45,   4,  20, 118, 215,  84, 118, 118, 202, 247, 200, 100, 222,\n",
       "       211, 228,  45, 107,  94,  73, 179, 158, 247, 188, 244,  96, 244,\n",
       "        30, 252, 133,   4, 114, 190, 164,  12,  67, 226,  82, 176,  90,\n",
       "        57, 149, 108, 245, 173,  37,  51, 129,  32, 209, 250,   9,  21,\n",
       "       164,  18, 227,  37, 205,  46, 140, 136, 110, 167,  81, 100, 156,\n",
       "       195,   7, 254, 119,   7,  94, 125,   7, 134, 201,  12, 152, 236,\n",
       "        92,  38, 120,  79, 109, 136, 148,   0])"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 1\n  y sizes: 255\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3r/hfw5ds7x10s2p6ps2ct8m4j40000gn/T/ipykernel_53136/3919752798.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training cell. History variable contains all the loss and accuracy at each step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVERBOSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1132\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m       \u001b[0;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m       data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[1;32m   1135\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m           \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m     self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1139\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;31m# If batch_size is not passed but steps is, calculate from the input data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1647\u001b[0m           label, \", \".join(str(i.shape[0]) for i in tf.nest.flatten(single_data)))\n\u001b[1;32m   1648\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1649\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 1\n  y sizes: 255\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "# Training cell. History variable contains all the loss and accuracy at each step.\n",
    "\n",
    "history = model.fit(context, targets, epochs = EPOCHS, verbose=VERBOSE, callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variable word_embeddings contains all the embeddings\n",
    "\n",
    "word_embeddings = embedding_layer.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.6219815,  1.6223835,  1.6204896,  1.614917 ,  1.617617 ,\n",
       "       -1.619683 ,  1.6071986, -1.6152487], dtype=float32)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for line in file_details['line_words']:\n",
    "    for wd in line:\n",
    "        words.append(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([49,  7, 24, 11, 25, 37, 49, 24, 35, 28, 33, 35,  8,  4, 23, 21, 22,\n",
       "       45, 35, 51, 48,  6, 27, 42, 49, 13, 39, 35, 11, 49, 13, 28, 33, 10,\n",
       "       51, 24, 47, 16,  3, 29, 24, 26, 36,  1, 24,  3,  4, 40, 20,  8,  1,\n",
       "       37,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0], dtype=int32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_details['padded_context'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'linguistics',\n",
       " 'word',\n",
       " 'embeddings',\n",
       " 'were',\n",
       " 'discussed',\n",
       " 'in',\n",
       " 'the',\n",
       " 'research',\n",
       " 'area',\n",
       " 'of',\n",
       " 'distributional',\n",
       " 'semantics.',\n",
       " 'It',\n",
       " 'aims',\n",
       " 'to',\n",
       " 'quantify',\n",
       " 'and',\n",
       " 'categorize',\n",
       " 'semantic',\n",
       " 'similarities',\n",
       " 'between',\n",
       " 'linguistic',\n",
       " 'items',\n",
       " 'based',\n",
       " 'on',\n",
       " 'their',\n",
       " 'distributional',\n",
       " 'properties',\n",
       " 'in',\n",
       " 'large',\n",
       " 'samples',\n",
       " 'of',\n",
       " 'language',\n",
       " 'data.',\n",
       " 'The',\n",
       " 'underlying',\n",
       " 'idea',\n",
       " 'that',\n",
       " '\"a',\n",
       " 'word',\n",
       " 'is',\n",
       " 'characterized',\n",
       " 'by',\n",
       " 'the',\n",
       " 'company',\n",
       " 'it',\n",
       " 'keeps\"',\n",
       " 'was',\n",
       " 'popularized',\n",
       " 'by',\n",
       " 'Firth.',\n",
       " 'The',\n",
       " 'technique',\n",
       " 'of',\n",
       " 'representing',\n",
       " 'words',\n",
       " 'as',\n",
       " 'vectors',\n",
       " 'has',\n",
       " 'roots',\n",
       " 'in',\n",
       " 'the',\n",
       " '1960s',\n",
       " 'with',\n",
       " 'the',\n",
       " 'development',\n",
       " 'of',\n",
       " 'the',\n",
       " 'vector',\n",
       " 'space',\n",
       " 'model',\n",
       " 'for',\n",
       " 'information',\n",
       " 'retrieval.',\n",
       " 'Reducing',\n",
       " 'the',\n",
       " 'number',\n",
       " 'of',\n",
       " 'dimensions',\n",
       " 'using',\n",
       " 'singular',\n",
       " 'value',\n",
       " 'decomposition',\n",
       " 'then',\n",
       " 'led',\n",
       " 'to',\n",
       " 'the',\n",
       " 'introduction',\n",
       " 'of',\n",
       " 'latent',\n",
       " 'semantic',\n",
       " 'analysis',\n",
       " 'in',\n",
       " 'the',\n",
       " 'late',\n",
       " '1980s.In',\n",
       " '2000',\n",
       " 'Bengio',\n",
       " 'et',\n",
       " 'al.',\n",
       " 'provided',\n",
       " 'in',\n",
       " 'a',\n",
       " 'series',\n",
       " 'of',\n",
       " 'papers',\n",
       " 'the',\n",
       " '\"Neural',\n",
       " 'probabilistic',\n",
       " 'language',\n",
       " 'models\"',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'the',\n",
       " 'high',\n",
       " 'dimensionality',\n",
       " 'of',\n",
       " 'words',\n",
       " 'representations',\n",
       " 'in',\n",
       " 'contexts',\n",
       " 'by',\n",
       " '\"learning',\n",
       " 'a',\n",
       " 'distributed',\n",
       " 'representation',\n",
       " 'for',\n",
       " 'words\".',\n",
       " '(Bengio',\n",
       " 'et',\n",
       " 'al,',\n",
       " '2003).',\n",
       " 'Word',\n",
       " 'embeddings',\n",
       " 'come',\n",
       " 'in',\n",
       " 'two',\n",
       " 'different',\n",
       " 'styles,',\n",
       " 'one',\n",
       " 'in',\n",
       " 'which',\n",
       " 'words',\n",
       " 'are',\n",
       " 'expressed',\n",
       " 'as',\n",
       " 'vectors',\n",
       " 'of',\n",
       " 'co-occurring',\n",
       " 'words,',\n",
       " 'and',\n",
       " 'another',\n",
       " 'in',\n",
       " 'which',\n",
       " 'words',\n",
       " 'are',\n",
       " 'expressed',\n",
       " 'as',\n",
       " 'vectors',\n",
       " 'of',\n",
       " 'linguistic',\n",
       " 'contexts',\n",
       " 'in',\n",
       " 'which',\n",
       " 'the',\n",
       " 'words',\n",
       " 'occur;',\n",
       " 'these',\n",
       " 'different',\n",
       " 'styles',\n",
       " 'are',\n",
       " 'studied',\n",
       " 'in',\n",
       " '(Lavelli',\n",
       " 'et',\n",
       " 'al,',\n",
       " '2004).',\n",
       " 'Roweis',\n",
       " 'and',\n",
       " 'Saul',\n",
       " 'published',\n",
       " 'in',\n",
       " 'Science',\n",
       " 'how',\n",
       " 'to',\n",
       " 'use',\n",
       " '\"locally',\n",
       " 'linear',\n",
       " 'embedding\"',\n",
       " '(LLE)',\n",
       " 'to',\n",
       " 'discover',\n",
       " 'representations',\n",
       " 'of',\n",
       " 'high',\n",
       " 'dimensional',\n",
       " 'data',\n",
       " 'structures.',\n",
       " 'The',\n",
       " 'area',\n",
       " 'developed',\n",
       " 'gradually',\n",
       " 'and',\n",
       " 'really',\n",
       " 'took',\n",
       " 'off',\n",
       " 'after',\n",
       " '2010,',\n",
       " 'partly',\n",
       " 'because',\n",
       " 'important',\n",
       " 'advances',\n",
       " 'had',\n",
       " 'been',\n",
       " 'made',\n",
       " 'since',\n",
       " 'then',\n",
       " 'on',\n",
       " 'the',\n",
       " 'quality',\n",
       " 'of',\n",
       " 'vectors',\n",
       " 'and',\n",
       " 'the',\n",
       " 'training',\n",
       " 'speed',\n",
       " 'of',\n",
       " 'the',\n",
       " 'model.',\n",
       " 'There',\n",
       " 'are',\n",
       " 'many',\n",
       " 'branches',\n",
       " 'and',\n",
       " 'many',\n",
       " 'research',\n",
       " 'groups',\n",
       " 'working',\n",
       " 'on',\n",
       " 'word',\n",
       " 'embeddings.',\n",
       " 'In',\n",
       " '2013,',\n",
       " 'a',\n",
       " 'team',\n",
       " 'at',\n",
       " 'Google',\n",
       " 'led',\n",
       " 'by',\n",
       " 'Tomas',\n",
       " 'Mikolov',\n",
       " 'created',\n",
       " 'word2vec,',\n",
       " 'a',\n",
       " 'word',\n",
       " 'embedding',\n",
       " 'toolkit',\n",
       " 'which',\n",
       " 'can',\n",
       " 'train',\n",
       " 'vector',\n",
       " 'space',\n",
       " 'models',\n",
       " 'faster',\n",
       " 'than',\n",
       " 'the',\n",
       " 'previous',\n",
       " 'approaches.',\n",
       " 'Most',\n",
       " 'new',\n",
       " 'word',\n",
       " 'embedding',\n",
       " 'techniques',\n",
       " 'rely',\n",
       " 'on',\n",
       " 'a',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'architecture',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'more',\n",
       " 'traditional',\n",
       " 'n-gram',\n",
       " 'models',\n",
       " 'and',\n",
       " 'unsupervised',\n",
       " 'learning.',\n",
       " 'One',\n",
       " 'of',\n",
       " 'the',\n",
       " 'main',\n",
       " 'limitations',\n",
       " 'of',\n",
       " 'word',\n",
       " 'embeddings',\n",
       " '(word',\n",
       " 'vector',\n",
       " 'space',\n",
       " 'models',\n",
       " 'in',\n",
       " 'general)',\n",
       " 'is',\n",
       " 'that',\n",
       " 'possible',\n",
       " 'meanings',\n",
       " 'of',\n",
       " 'a',\n",
       " 'word',\n",
       " 'are',\n",
       " 'conflated',\n",
       " 'into',\n",
       " 'a',\n",
       " 'single',\n",
       " 'representation',\n",
       " '(a',\n",
       " 'single',\n",
       " 'vector',\n",
       " 'in',\n",
       " 'the',\n",
       " 'semantic',\n",
       " 'space).',\n",
       " 'Sense',\n",
       " 'embeddings',\n",
       " 'are',\n",
       " 'a',\n",
       " 'solution',\n",
       " 'to',\n",
       " 'this',\n",
       " 'problem:',\n",
       " 'individual',\n",
       " 'meanings',\n",
       " 'of',\n",
       " 'words',\n",
       " 'are',\n",
       " 'represented',\n",
       " 'as',\n",
       " 'distinct',\n",
       " 'vectors',\n",
       " 'in',\n",
       " 'the',\n",
       " 'space.',\n",
       " 'For',\n",
       " 'biological',\n",
       " 'sequences:',\n",
       " 'BioVectors',\n",
       " 'Word',\n",
       " 'embeddings',\n",
       " 'for',\n",
       " 'n-grams',\n",
       " 'in',\n",
       " 'biological',\n",
       " 'sequences',\n",
       " '(e.g.',\n",
       " 'DNA,',\n",
       " 'RNA,',\n",
       " 'and',\n",
       " 'Proteins)',\n",
       " 'for',\n",
       " 'bioinformatics',\n",
       " 'applications',\n",
       " 'have',\n",
       " 'been',\n",
       " 'proposed',\n",
       " 'by',\n",
       " 'Asgari',\n",
       " 'and',\n",
       " 'Mofrad.',\n",
       " 'Named',\n",
       " 'bio-vectors',\n",
       " '(BioVec)',\n",
       " 'to',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'biological',\n",
       " 'sequences',\n",
       " 'in',\n",
       " 'general',\n",
       " 'with',\n",
       " 'protein-vectors',\n",
       " '(ProtVec)',\n",
       " 'for',\n",
       " 'proteins',\n",
       " '(amino-acid',\n",
       " 'sequences)',\n",
       " 'and',\n",
       " 'gene-vectors',\n",
       " '(GeneVec)',\n",
       " 'for',\n",
       " 'gene',\n",
       " 'sequences,',\n",
       " 'this',\n",
       " 'representation',\n",
       " 'can',\n",
       " 'be',\n",
       " 'widely',\n",
       " 'used',\n",
       " 'in',\n",
       " 'applications',\n",
       " 'of',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'in',\n",
       " 'proteomics',\n",
       " 'and',\n",
       " 'genomics.',\n",
       " 'The',\n",
       " 'results',\n",
       " 'presented',\n",
       " 'by',\n",
       " 'Asgari',\n",
       " 'and',\n",
       " 'Mofrad',\n",
       " 'suggest',\n",
       " 'that',\n",
       " 'BioVectors',\n",
       " 'can',\n",
       " 'characterize',\n",
       " 'biological',\n",
       " 'sequences',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'biochemical',\n",
       " 'and',\n",
       " 'biophysical',\n",
       " 'interpretations',\n",
       " 'of',\n",
       " 'the',\n",
       " 'underlying',\n",
       " 'patterns.',\n",
       " 'Thought',\n",
       " 'vectors',\n",
       " 'Thought',\n",
       " 'vectors',\n",
       " 'are',\n",
       " 'an',\n",
       " 'extension',\n",
       " 'of',\n",
       " 'word',\n",
       " 'embeddings',\n",
       " 'to',\n",
       " 'entire',\n",
       " 'sentences',\n",
       " 'or',\n",
       " 'even',\n",
       " 'documents.',\n",
       " 'Some',\n",
       " 'researchers',\n",
       " 'hope',\n",
       " 'that',\n",
       " 'these',\n",
       " 'can',\n",
       " 'improve',\n",
       " 'the',\n",
       " 'quality',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'translation.',\n",
       " 'Software',\n",
       " 'for',\n",
       " 'training',\n",
       " 'and',\n",
       " 'using',\n",
       " 'word',\n",
       " 'embeddings',\n",
       " 'includes',\n",
       " 'Tomas',\n",
       " \"Mikolov's\",\n",
       " 'Word2vec,',\n",
       " 'Stanford',\n",
       " \"University's\",\n",
       " 'GloVe,',\n",
       " \"AllenNLP's\",\n",
       " 'Elmo,fastText,',\n",
       " 'Gensim,',\n",
       " 'Indra',\n",
       " 'and',\n",
       " 'Deeplearning4j.',\n",
       " 'Principal',\n",
       " 'Component',\n",
       " 'Analysis',\n",
       " '(PCA)',\n",
       " 'and',\n",
       " 'T-Distributed',\n",
       " 'Stochastic',\n",
       " 'Neighbour',\n",
       " 'Embedding',\n",
       " '(t-SNE)',\n",
       " 'are',\n",
       " 'both',\n",
       " 'used',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'the',\n",
       " 'dimensionality',\n",
       " 'of',\n",
       " 'word',\n",
       " 'vector',\n",
       " 'spaces',\n",
       " 'and',\n",
       " 'visualize',\n",
       " 'word',\n",
       " 'embeddings',\n",
       " 'and',\n",
       " 'clusters.',\n",
       " 'Examples',\n",
       " 'of',\n",
       " 'application',\n",
       " 'For',\n",
       " 'instance,',\n",
       " 'the',\n",
       " 'fastText',\n",
       " 'is',\n",
       " 'also',\n",
       " 'used',\n",
       " 'to',\n",
       " 'calculate',\n",
       " 'word',\n",
       " 'embeddings',\n",
       " 'for',\n",
       " 'text',\n",
       " 'corpora',\n",
       " 'in',\n",
       " 'Sketch',\n",
       " 'Engine',\n",
       " 'that',\n",
       " 'are',\n",
       " 'available',\n",
       " 'online.']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the searched line and word indeces\n",
    "def get_indices(word):\n",
    "    line_index = 0\n",
    "    word_index = 0\n",
    "    for line in file_details['line_words']:\n",
    "        for wrd in line:\n",
    "            if wrd==word:\n",
    "                return (line_index, word_index)\n",
    "            word_index = word_index + 1\n",
    "        line_index = line_index + 1\n",
    "    return (-1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the word embeddings\n",
    "\n",
    "def fetch_embedding(line_index, word_index, word_embeddings):\n",
    "    return word_embeddings[line_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the least distant word with the help of word_embeddings. \n",
    "# given a word and the learned word_embeddings, this function will return the next (closest) word.\n",
    "# evaluation_method can be cosine or eculedian \n",
    "\n",
    "def closest_word(word, word_embeddings, evaluation_method):\n",
    "    line_index, word_index = get_indices(word)\n",
    "    print(word_embeddings.shape)\n",
    "    if (line_index == -1) or (word_index == -1):\n",
    "        print(\"searched word not found in the given context!\")\n",
    "    else:\n",
    "        embedding = fetch_embedding(line_index, word_index, word_embeddings)\n",
    "        for embd in word_embeddings:\n",
    "            result = 1 - spatial.distance.cosine(embedding, embd)\n",
    "            print(result, line_index, word_index)\n",
    "    return \"word\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 8)\n",
      "0.4817196726799011 1 223\n",
      "1 1 223\n",
      "-0.019669702276587486 1 223\n",
      "0.4816100597381592 1 223\n",
      "-0.2703162133693695 1 223\n",
      "0.9984984993934631 1 223\n",
      "0.25284287333488464 1 223\n",
      "0.007079429924488068 1 223\n",
      "0.024043571203947067 1 223\n",
      "-0.2701837718486786 1 223\n",
      "-0.026256121695041656 1 223\n",
      "-0.041422128677368164 1 223\n",
      "0.4505995213985443 1 223\n",
      "-0.728593647480011 1 223\n",
      "0.48598986864089966 1 223\n",
      "0.2297157496213913 1 223\n",
      "-0.2341039627790451 1 223\n",
      "0.273592084646225 1 223\n",
      "0.24298851191997528 1 223\n",
      "-0.03774547576904297 1 223\n",
      "0.23756183683872223 1 223\n",
      "0.47415846586227417 1 223\n",
      "0.46532416343688965 1 223\n",
      "0.5141133666038513 1 223\n",
      "0.47594621777534485 1 223\n",
      "-0.4893186092376709 1 223\n",
      "0.7397944331169128 1 223\n",
      "0.5194547772407532 1 223\n",
      "0.2529175579547882 1 223\n",
      "0.2443699836730957 1 223\n",
      "-0.4822525382041931 1 223\n",
      "0.27850496768951416 1 223\n",
      "0.5336160659790039 1 223\n",
      "0.47386428713798523 1 223\n",
      "0.7442044019699097 1 223\n",
      "-0.026949871331453323 1 223\n",
      "-0.02250630594789982 1 223\n",
      "0.6057190895080566 1 223\n",
      "0.22758036851882935 1 223\n",
      "0.5950136184692383 1 223\n",
      "0.23752805590629578 1 223\n",
      "0.2036053091287613 1 223\n",
      "0.23003095388412476 1 223\n",
      "0.013133385218679905 1 223\n",
      "0.23218680918216705 1 223\n",
      "0.22721469402313232 1 223\n",
      "0.47702333331108093 1 223\n",
      "0.46682241559028625 1 223\n",
      "0.4763714075088501 1 223\n",
      "0.7394258379936218 1 223\n",
      "0.013109130784869194 1 223\n",
      "0.014931303448975086 1 223\n",
      "0.2434321641921997 1 223\n",
      "-0.0369514524936676 1 223\n",
      "0.23518896102905273 1 223\n",
      "0.21473746001720428 1 223\n",
      "0.48250624537467957 1 223\n",
      "0.49049311876296997 1 223\n",
      "0.23383422195911407 1 223\n",
      "-0.735927402973175 1 223\n",
      "-0.044184304773807526 1 223\n",
      "0.172098308801651 1 223\n",
      "-0.031179899349808693 1 223\n",
      "0.4875829815864563 1 223\n",
      "0.020294159650802612 1 223\n",
      "-0.017669517546892166 1 223\n",
      "0.23725926876068115 1 223\n",
      "-0.23859311640262604 1 223\n",
      "-0.2150551974773407 1 223\n",
      "0.2678702771663666 1 223\n",
      "0.7424094080924988 1 223\n",
      "-0.2790634334087372 1 223\n",
      "-0.24023441970348358 1 223\n",
      "0.49613526463508606 1 223\n",
      "0.4851842522621155 1 223\n",
      "0.2239542156457901 1 223\n",
      "-0.35324347019195557 1 223\n",
      "-0.7469438910484314 1 223\n",
      "0.49256083369255066 1 223\n",
      "0.2739667296409607 1 223\n",
      "-0.2346101850271225 1 223\n",
      "-0.6097637414932251 1 223\n",
      "0.7401242852210999 1 223\n",
      "0.2329016923904419 1 223\n",
      "0.003969946876168251 1 223\n",
      "-0.661765992641449 1 223\n",
      "0.5132002234458923 1 223\n",
      "-0.06448337435722351 1 223\n",
      "-0.31698575615882874 1 223\n",
      "0.23198924958705902 1 223\n",
      "-0.73714280128479 1 223\n",
      "0.2548845410346985 1 223\n",
      "-0.45342326164245605 1 223\n",
      "0.21375791728496552 1 223\n",
      "0.23981361091136932 1 223\n",
      "0.3270331025123596 1 223\n",
      "-0.21578100323677063 1 223\n",
      "0.2725815773010254 1 223\n",
      "0.42078906297683716 1 223\n",
      "-0.0171970184892416 1 223\n",
      "0.019321918487548828 1 223\n",
      "0.14219313859939575 1 223\n",
      "0.14384959638118744 1 223\n",
      "0.23278287053108215 1 223\n",
      "0.23241840302944183 1 223\n",
      "0.7487998604774475 1 223\n",
      "0.1556222289800644 1 223\n",
      "-0.0019976261537522078 1 223\n",
      "0.3853144943714142 1 223\n",
      "-0.33307239413261414 1 223\n",
      "-0.4183133542537689 1 223\n",
      "-0.12872257828712463 1 223\n",
      "0.016445621848106384 1 223\n",
      "-0.42199403047561646 1 223\n",
      "-0.32976043224334717 1 223\n",
      "0.4873337149620056 1 223\n",
      "0.2256854772567749 1 223\n",
      "-0.14226920902729034 1 223\n",
      "-0.2212175726890564 1 223\n",
      "-0.280664324760437 1 223\n",
      "-0.00974131841212511 1 223\n",
      "0.22914569079875946 1 223\n",
      "-0.016801247373223305 1 223\n",
      "-0.26719677448272705 1 223\n",
      "-0.013696831651031971 1 223\n",
      "0.0029954363126307726 1 223\n",
      "-0.17215511202812195 1 223\n",
      "-0.1889214962720871 1 223\n",
      "-0.26343345642089844 1 223\n",
      "0.1971452385187149 1 223\n",
      "0.6775342226028442 1 223\n",
      "-0.37100982666015625 1 223\n",
      "0.011050621047616005 1 223\n",
      "-0.025125885382294655 1 223\n",
      "-0.23417872190475464 1 223\n",
      "-0.26743626594543457 1 223\n",
      "0.0901363343000412 1 223\n",
      "-0.2766701877117157 1 223\n",
      "0.23458446562290192 1 223\n",
      "0.5172548890113831 1 223\n",
      "-0.17936019599437714 1 223\n",
      "0.19332177937030792 1 223\n",
      "0.01680639572441578 1 223\n",
      "-0.2980683147907257 1 223\n",
      "0.5318514108657837 1 223\n",
      "0.011049975641071796 1 223\n",
      "-0.017056480050086975 1 223\n",
      "0.27590927481651306 1 223\n",
      "0.2242591828107834 1 223\n",
      "-0.024856535717844963 1 223\n",
      "-0.4607062041759491 1 223\n",
      "0.22011364996433258 1 223\n",
      "-0.0602443628013134 1 223\n",
      "-0.026767071336507797 1 223\n",
      "-0.3389652371406555 1 223\n",
      "-0.33903804421424866 1 223\n",
      "0.2837851941585541 1 223\n",
      "0.4568912386894226 1 223\n",
      "0.48388442397117615 1 223\n",
      "-0.24539503455162048 1 223\n",
      "0.27434980869293213 1 223\n",
      "0.8661559820175171 1 223\n",
      "-0.14227665960788727 1 223\n",
      "0.017919564619660378 1 223\n",
      "0.8756438493728638 1 223\n",
      "-0.019810963422060013 1 223\n",
      "0.5274453163146973 1 223\n",
      "-0.4065234959125519 1 223\n",
      "0.4926261901855469 1 223\n",
      "0.558775782585144 1 223\n",
      "0.504511833190918 1 223\n",
      "0.28106406331062317 1 223\n",
      "0.24061407148838043 1 223\n",
      "0.03176528960466385 1 223\n",
      "0.1283930540084839 1 223\n",
      "-0.48896324634552 1 223\n",
      "0.2661016285419464 1 223\n",
      "0.03367890790104866 1 223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'word'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_word(\"2000\", word_embeddings, 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.6219815,  1.6223835,  1.6204896, ..., -1.619683 ,  1.6071986,\n",
       "        -1.6152487],\n",
       "       [-1.6241891, -1.6635616,  1.6017293, ..., -1.5814848,  1.3999026,\n",
       "         1.6283284],\n",
       "       [ 1.5662514, -1.6116898, -1.5478582, ..., -1.5593542,  1.4303602,\n",
       "        -1.6137869],\n",
       "       ...,\n",
       "       [-1.6226442,  1.6031357, -1.6311088, ...,  1.6225965, -1.5842654,\n",
       "        -1.6153655],\n",
       "       [-1.5912836,  1.6120912,  1.6211563, ..., -1.6162713, -1.586357 ,\n",
       "         1.5927837],\n",
       "       [-1.6265788, -1.6375501,  1.6025525, ...,  1.5824865, -1.5684392,\n",
       "         1.622879 ]], dtype=float32)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_details['line_words'][0][177]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_details['padded_context'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lines': ['In linguistics word embeddings were discussed in the research area of distributional semantics. It aims to quantify and categorize semantic similarities between linguistic items based on their distributional properties in large samples of language data. The underlying idea that \"a word is characterized by the company it keeps\" was popularized by Firth.',\n",
       "  'The technique of representing words as vectors has roots in the 1960s with the development of the vector space model for information retrieval. Reducing the number of dimensions using singular value decomposition then led to the introduction of latent semantic analysis in the late 1980s.In 2000 Bengio et al. provided in a series of papers the \"Neural probabilistic language models\" to reduce the high dimensionality of words representations in contexts by \"learning a distributed representation for words\". (Bengio et al, 2003). Word embeddings come in two different styles, one in which words are expressed as vectors of co-occurring words, and another in which words are expressed as vectors of linguistic contexts in which the words occur; these different styles are studied in (Lavelli et al, 2004). Roweis and Saul published in Science how to use \"locally linear embedding\" (LLE) to discover representations of high dimensional data structures. The area developed gradually and really took off after 2010, partly because important advances had been made since then on the quality of vectors and the training speed of the model.',\n",
       "  'There are many branches and many research groups working on word embeddings. In 2013, a team at Google led by Tomas Mikolov created word2vec, a word embedding toolkit which can train vector space models faster than the previous approaches. Most new word embedding techniques rely on a neural network architecture instead of more traditional n-gram models and unsupervised learning.',\n",
       "  'Limitations',\n",
       "  'One of the main limitations of word embeddings (word vector space models in general) is that possible meanings of a word are conflated into a single representation (a single vector in the semantic space). Sense embeddings are a solution to this problem: individual meanings of words are represented as distinct vectors in the space.',\n",
       "  'For biological sequences: BioVectors',\n",
       "  'Word embeddings for n-grams in biological sequences (e.g. DNA, RNA, and Proteins) for bioinformatics applications have been proposed by Asgari and Mofrad. Named bio-vectors (BioVec) to refer to biological sequences in general with protein-vectors (ProtVec) for proteins (amino-acid sequences) and gene-vectors (GeneVec) for gene sequences, this representation can be widely used in applications of deep learning in proteomics and genomics. The results presented by Asgari and Mofrad suggest that BioVectors can characterize biological sequences in terms of biochemical and biophysical interpretations of the underlying patterns.',\n",
       "  'Thought vectors',\n",
       "  'Thought vectors are an extension of word embeddings to entire sentences or even documents. Some researchers hope that these can improve the quality of machine translation.',\n",
       "  'Software',\n",
       "  \"Software for training and using word embeddings includes Tomas Mikolov's Word2vec, Stanford University's GloVe, AllenNLP's Elmo,fastText, Gensim, Indra and Deeplearning4j. Principal Component Analysis (PCA) and T-Distributed Stochastic Neighbour Embedding (t-SNE) are both used to reduce the dimensionality of word vector spaces and visualize word embeddings and clusters.\",\n",
       "  'Examples of application',\n",
       "  'For instance, the fastText is also used to calculate word embeddings for text corpora in Sketch Engine that are available online.'],\n",
       " 'line_words': [['In',\n",
       "   'linguistics',\n",
       "   'word',\n",
       "   'embeddings',\n",
       "   'were',\n",
       "   'discussed',\n",
       "   'in',\n",
       "   'the',\n",
       "   'research',\n",
       "   'area',\n",
       "   'of',\n",
       "   'distributional',\n",
       "   'semantics.',\n",
       "   'It',\n",
       "   'aims',\n",
       "   'to',\n",
       "   'quantify',\n",
       "   'and',\n",
       "   'categorize',\n",
       "   'semantic',\n",
       "   'similarities',\n",
       "   'between',\n",
       "   'linguistic',\n",
       "   'items',\n",
       "   'based',\n",
       "   'on',\n",
       "   'their',\n",
       "   'distributional',\n",
       "   'properties',\n",
       "   'in',\n",
       "   'large',\n",
       "   'samples',\n",
       "   'of',\n",
       "   'language',\n",
       "   'data.',\n",
       "   'The',\n",
       "   'underlying',\n",
       "   'idea',\n",
       "   'that',\n",
       "   '\"a',\n",
       "   'word',\n",
       "   'is',\n",
       "   'characterized',\n",
       "   'by',\n",
       "   'the',\n",
       "   'company',\n",
       "   'it',\n",
       "   'keeps\"',\n",
       "   'was',\n",
       "   'popularized',\n",
       "   'by',\n",
       "   'Firth.'],\n",
       "  ['The',\n",
       "   'technique',\n",
       "   'of',\n",
       "   'representing',\n",
       "   'words',\n",
       "   'as',\n",
       "   'vectors',\n",
       "   'has',\n",
       "   'roots',\n",
       "   'in',\n",
       "   'the',\n",
       "   '1960s',\n",
       "   'with',\n",
       "   'the',\n",
       "   'development',\n",
       "   'of',\n",
       "   'the',\n",
       "   'vector',\n",
       "   'space',\n",
       "   'model',\n",
       "   'for',\n",
       "   'information',\n",
       "   'retrieval.',\n",
       "   'Reducing',\n",
       "   'the',\n",
       "   'number',\n",
       "   'of',\n",
       "   'dimensions',\n",
       "   'using',\n",
       "   'singular',\n",
       "   'value',\n",
       "   'decomposition',\n",
       "   'then',\n",
       "   'led',\n",
       "   'to',\n",
       "   'the',\n",
       "   'introduction',\n",
       "   'of',\n",
       "   'latent',\n",
       "   'semantic',\n",
       "   'analysis',\n",
       "   'in',\n",
       "   'the',\n",
       "   'late',\n",
       "   '1980s.In',\n",
       "   '2000',\n",
       "   'Bengio',\n",
       "   'et',\n",
       "   'al.',\n",
       "   'provided',\n",
       "   'in',\n",
       "   'a',\n",
       "   'series',\n",
       "   'of',\n",
       "   'papers',\n",
       "   'the',\n",
       "   '\"Neural',\n",
       "   'probabilistic',\n",
       "   'language',\n",
       "   'models\"',\n",
       "   'to',\n",
       "   'reduce',\n",
       "   'the',\n",
       "   'high',\n",
       "   'dimensionality',\n",
       "   'of',\n",
       "   'words',\n",
       "   'representations',\n",
       "   'in',\n",
       "   'contexts',\n",
       "   'by',\n",
       "   '\"learning',\n",
       "   'a',\n",
       "   'distributed',\n",
       "   'representation',\n",
       "   'for',\n",
       "   'words\".',\n",
       "   '(Bengio',\n",
       "   'et',\n",
       "   'al,',\n",
       "   '2003).',\n",
       "   'Word',\n",
       "   'embeddings',\n",
       "   'come',\n",
       "   'in',\n",
       "   'two',\n",
       "   'different',\n",
       "   'styles,',\n",
       "   'one',\n",
       "   'in',\n",
       "   'which',\n",
       "   'words',\n",
       "   'are',\n",
       "   'expressed',\n",
       "   'as',\n",
       "   'vectors',\n",
       "   'of',\n",
       "   'co-occurring',\n",
       "   'words,',\n",
       "   'and',\n",
       "   'another',\n",
       "   'in',\n",
       "   'which',\n",
       "   'words',\n",
       "   'are',\n",
       "   'expressed',\n",
       "   'as',\n",
       "   'vectors',\n",
       "   'of',\n",
       "   'linguistic',\n",
       "   'contexts',\n",
       "   'in',\n",
       "   'which',\n",
       "   'the',\n",
       "   'words',\n",
       "   'occur;',\n",
       "   'these',\n",
       "   'different',\n",
       "   'styles',\n",
       "   'are',\n",
       "   'studied',\n",
       "   'in',\n",
       "   '(Lavelli',\n",
       "   'et',\n",
       "   'al,',\n",
       "   '2004).',\n",
       "   'Roweis',\n",
       "   'and',\n",
       "   'Saul',\n",
       "   'published',\n",
       "   'in',\n",
       "   'Science',\n",
       "   'how',\n",
       "   'to',\n",
       "   'use',\n",
       "   '\"locally',\n",
       "   'linear',\n",
       "   'embedding\"',\n",
       "   '(LLE)',\n",
       "   'to',\n",
       "   'discover',\n",
       "   'representations',\n",
       "   'of',\n",
       "   'high',\n",
       "   'dimensional',\n",
       "   'data',\n",
       "   'structures.',\n",
       "   'The',\n",
       "   'area',\n",
       "   'developed',\n",
       "   'gradually',\n",
       "   'and',\n",
       "   'really',\n",
       "   'took',\n",
       "   'off',\n",
       "   'after',\n",
       "   '2010,',\n",
       "   'partly',\n",
       "   'because',\n",
       "   'important',\n",
       "   'advances',\n",
       "   'had',\n",
       "   'been',\n",
       "   'made',\n",
       "   'since',\n",
       "   'then',\n",
       "   'on',\n",
       "   'the',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'vectors',\n",
       "   'and',\n",
       "   'the',\n",
       "   'training',\n",
       "   'speed',\n",
       "   'of',\n",
       "   'the',\n",
       "   'model.'],\n",
       "  ['There',\n",
       "   'are',\n",
       "   'many',\n",
       "   'branches',\n",
       "   'and',\n",
       "   'many',\n",
       "   'research',\n",
       "   'groups',\n",
       "   'working',\n",
       "   'on',\n",
       "   'word',\n",
       "   'embeddings.',\n",
       "   'In',\n",
       "   '2013,',\n",
       "   'a',\n",
       "   'team',\n",
       "   'at',\n",
       "   'Google',\n",
       "   'led',\n",
       "   'by',\n",
       "   'Tomas',\n",
       "   'Mikolov',\n",
       "   'created',\n",
       "   'word2vec,',\n",
       "   'a',\n",
       "   'word',\n",
       "   'embedding',\n",
       "   'toolkit',\n",
       "   'which',\n",
       "   'can',\n",
       "   'train',\n",
       "   'vector',\n",
       "   'space',\n",
       "   'models',\n",
       "   'faster',\n",
       "   'than',\n",
       "   'the',\n",
       "   'previous',\n",
       "   'approaches.',\n",
       "   'Most',\n",
       "   'new',\n",
       "   'word',\n",
       "   'embedding',\n",
       "   'techniques',\n",
       "   'rely',\n",
       "   'on',\n",
       "   'a',\n",
       "   'neural',\n",
       "   'network',\n",
       "   'architecture',\n",
       "   'instead',\n",
       "   'of',\n",
       "   'more',\n",
       "   'traditional',\n",
       "   'n-gram',\n",
       "   'models',\n",
       "   'and',\n",
       "   'unsupervised',\n",
       "   'learning.'],\n",
       "  [],\n",
       "  ['One',\n",
       "   'of',\n",
       "   'the',\n",
       "   'main',\n",
       "   'limitations',\n",
       "   'of',\n",
       "   'word',\n",
       "   'embeddings',\n",
       "   '(word',\n",
       "   'vector',\n",
       "   'space',\n",
       "   'models',\n",
       "   'in',\n",
       "   'general)',\n",
       "   'is',\n",
       "   'that',\n",
       "   'possible',\n",
       "   'meanings',\n",
       "   'of',\n",
       "   'a',\n",
       "   'word',\n",
       "   'are',\n",
       "   'conflated',\n",
       "   'into',\n",
       "   'a',\n",
       "   'single',\n",
       "   'representation',\n",
       "   '(a',\n",
       "   'single',\n",
       "   'vector',\n",
       "   'in',\n",
       "   'the',\n",
       "   'semantic',\n",
       "   'space).',\n",
       "   'Sense',\n",
       "   'embeddings',\n",
       "   'are',\n",
       "   'a',\n",
       "   'solution',\n",
       "   'to',\n",
       "   'this',\n",
       "   'problem:',\n",
       "   'individual',\n",
       "   'meanings',\n",
       "   'of',\n",
       "   'words',\n",
       "   'are',\n",
       "   'represented',\n",
       "   'as',\n",
       "   'distinct',\n",
       "   'vectors',\n",
       "   'in',\n",
       "   'the',\n",
       "   'space.'],\n",
       "  ['For', 'biological', 'sequences:', 'BioVectors'],\n",
       "  ['Word',\n",
       "   'embeddings',\n",
       "   'for',\n",
       "   'n-grams',\n",
       "   'in',\n",
       "   'biological',\n",
       "   'sequences',\n",
       "   '(e.g.',\n",
       "   'DNA,',\n",
       "   'RNA,',\n",
       "   'and',\n",
       "   'Proteins)',\n",
       "   'for',\n",
       "   'bioinformatics',\n",
       "   'applications',\n",
       "   'have',\n",
       "   'been',\n",
       "   'proposed',\n",
       "   'by',\n",
       "   'Asgari',\n",
       "   'and',\n",
       "   'Mofrad.',\n",
       "   'Named',\n",
       "   'bio-vectors',\n",
       "   '(BioVec)',\n",
       "   'to',\n",
       "   'refer',\n",
       "   'to',\n",
       "   'biological',\n",
       "   'sequences',\n",
       "   'in',\n",
       "   'general',\n",
       "   'with',\n",
       "   'protein-vectors',\n",
       "   '(ProtVec)',\n",
       "   'for',\n",
       "   'proteins',\n",
       "   '(amino-acid',\n",
       "   'sequences)',\n",
       "   'and',\n",
       "   'gene-vectors',\n",
       "   '(GeneVec)',\n",
       "   'for',\n",
       "   'gene',\n",
       "   'sequences,',\n",
       "   'this',\n",
       "   'representation',\n",
       "   'can',\n",
       "   'be',\n",
       "   'widely',\n",
       "   'used',\n",
       "   'in',\n",
       "   'applications',\n",
       "   'of',\n",
       "   'deep',\n",
       "   'learning',\n",
       "   'in',\n",
       "   'proteomics',\n",
       "   'and',\n",
       "   'genomics.',\n",
       "   'The',\n",
       "   'results',\n",
       "   'presented',\n",
       "   'by',\n",
       "   'Asgari',\n",
       "   'and',\n",
       "   'Mofrad',\n",
       "   'suggest',\n",
       "   'that',\n",
       "   'BioVectors',\n",
       "   'can',\n",
       "   'characterize',\n",
       "   'biological',\n",
       "   'sequences',\n",
       "   'in',\n",
       "   'terms',\n",
       "   'of',\n",
       "   'biochemical',\n",
       "   'and',\n",
       "   'biophysical',\n",
       "   'interpretations',\n",
       "   'of',\n",
       "   'the',\n",
       "   'underlying',\n",
       "   'patterns.'],\n",
       "  ['Thought', 'vectors'],\n",
       "  ['Thought',\n",
       "   'vectors',\n",
       "   'are',\n",
       "   'an',\n",
       "   'extension',\n",
       "   'of',\n",
       "   'word',\n",
       "   'embeddings',\n",
       "   'to',\n",
       "   'entire',\n",
       "   'sentences',\n",
       "   'or',\n",
       "   'even',\n",
       "   'documents.',\n",
       "   'Some',\n",
       "   'researchers',\n",
       "   'hope',\n",
       "   'that',\n",
       "   'these',\n",
       "   'can',\n",
       "   'improve',\n",
       "   'the',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'machine',\n",
       "   'translation.'],\n",
       "  [],\n",
       "  ['Software',\n",
       "   'for',\n",
       "   'training',\n",
       "   'and',\n",
       "   'using',\n",
       "   'word',\n",
       "   'embeddings',\n",
       "   'includes',\n",
       "   'Tomas',\n",
       "   \"Mikolov's\",\n",
       "   'Word2vec,',\n",
       "   'Stanford',\n",
       "   \"University's\",\n",
       "   'GloVe,',\n",
       "   \"AllenNLP's\",\n",
       "   'Elmo,fastText,',\n",
       "   'Gensim,',\n",
       "   'Indra',\n",
       "   'and',\n",
       "   'Deeplearning4j.',\n",
       "   'Principal',\n",
       "   'Component',\n",
       "   'Analysis',\n",
       "   '(PCA)',\n",
       "   'and',\n",
       "   'T-Distributed',\n",
       "   'Stochastic',\n",
       "   'Neighbour',\n",
       "   'Embedding',\n",
       "   '(t-SNE)',\n",
       "   'are',\n",
       "   'both',\n",
       "   'used',\n",
       "   'to',\n",
       "   'reduce',\n",
       "   'the',\n",
       "   'dimensionality',\n",
       "   'of',\n",
       "   'word',\n",
       "   'vector',\n",
       "   'spaces',\n",
       "   'and',\n",
       "   'visualize',\n",
       "   'word',\n",
       "   'embeddings',\n",
       "   'and',\n",
       "   'clusters.'],\n",
       "  ['Examples', 'of', 'application'],\n",
       "  ['For',\n",
       "   'instance,',\n",
       "   'the',\n",
       "   'fastText',\n",
       "   'is',\n",
       "   'also',\n",
       "   'used',\n",
       "   'to',\n",
       "   'calculate',\n",
       "   'word',\n",
       "   'embeddings',\n",
       "   'for',\n",
       "   'text',\n",
       "   'corpora',\n",
       "   'in',\n",
       "   'Sketch',\n",
       "   'Engine',\n",
       "   'that',\n",
       "   'are',\n",
       "   'available',\n",
       "   'online.']],\n",
       " 'encoded_line_words': [[49,\n",
       "   7,\n",
       "   24,\n",
       "   11,\n",
       "   25,\n",
       "   37,\n",
       "   49,\n",
       "   24,\n",
       "   35,\n",
       "   28,\n",
       "   33,\n",
       "   35,\n",
       "   8,\n",
       "   4,\n",
       "   23,\n",
       "   21,\n",
       "   22,\n",
       "   45,\n",
       "   35,\n",
       "   51,\n",
       "   48,\n",
       "   6,\n",
       "   27,\n",
       "   42,\n",
       "   49,\n",
       "   13,\n",
       "   39,\n",
       "   35,\n",
       "   11,\n",
       "   49,\n",
       "   13,\n",
       "   28,\n",
       "   33,\n",
       "   10,\n",
       "   51,\n",
       "   24,\n",
       "   47,\n",
       "   16,\n",
       "   3,\n",
       "   29,\n",
       "   24,\n",
       "   26,\n",
       "   36,\n",
       "   1,\n",
       "   24,\n",
       "   3,\n",
       "   4,\n",
       "   40,\n",
       "   20,\n",
       "   8,\n",
       "   1,\n",
       "   37],\n",
       "  [165,\n",
       "   90,\n",
       "   99,\n",
       "   94,\n",
       "   62,\n",
       "   121,\n",
       "   149,\n",
       "   74,\n",
       "   116,\n",
       "   133,\n",
       "   165,\n",
       "   116,\n",
       "   67,\n",
       "   165,\n",
       "   105,\n",
       "   99,\n",
       "   165,\n",
       "   34,\n",
       "   89,\n",
       "   72,\n",
       "   18,\n",
       "   172,\n",
       "   172,\n",
       "   38,\n",
       "   165,\n",
       "   97,\n",
       "   99,\n",
       "   43,\n",
       "   159,\n",
       "   32,\n",
       "   44,\n",
       "   158,\n",
       "   125,\n",
       "   7,\n",
       "   63,\n",
       "   165,\n",
       "   77,\n",
       "   99,\n",
       "   51,\n",
       "   177,\n",
       "   123,\n",
       "   133,\n",
       "   165,\n",
       "   25,\n",
       "   63,\n",
       "   66,\n",
       "   20,\n",
       "   137,\n",
       "   58,\n",
       "   168,\n",
       "   133,\n",
       "   146,\n",
       "   144,\n",
       "   99,\n",
       "   115,\n",
       "   165,\n",
       "   142,\n",
       "   168,\n",
       "   94,\n",
       "   35,\n",
       "   63,\n",
       "   71,\n",
       "   165,\n",
       "   151,\n",
       "   71,\n",
       "   99,\n",
       "   62,\n",
       "   125,\n",
       "   133,\n",
       "   15,\n",
       "   25,\n",
       "   58,\n",
       "   146,\n",
       "   132,\n",
       "   134,\n",
       "   18,\n",
       "   62,\n",
       "   20,\n",
       "   137,\n",
       "   58,\n",
       "   175,\n",
       "   30,\n",
       "   80,\n",
       "   104,\n",
       "   133,\n",
       "   45,\n",
       "   105,\n",
       "   10,\n",
       "   44,\n",
       "   133,\n",
       "   9,\n",
       "   62,\n",
       "   107,\n",
       "   103,\n",
       "   121,\n",
       "   149,\n",
       "   99,\n",
       "   133,\n",
       "   62,\n",
       "   90,\n",
       "   4,\n",
       "   133,\n",
       "   9,\n",
       "   62,\n",
       "   107,\n",
       "   103,\n",
       "   121,\n",
       "   149,\n",
       "   99,\n",
       "   63,\n",
       "   15,\n",
       "   133,\n",
       "   9,\n",
       "   165,\n",
       "   62,\n",
       "   134,\n",
       "   69,\n",
       "   105,\n",
       "   10,\n",
       "   107,\n",
       "   63,\n",
       "   133,\n",
       "   8,\n",
       "   137,\n",
       "   58,\n",
       "   100,\n",
       "   5,\n",
       "   90,\n",
       "   99,\n",
       "   166,\n",
       "   133,\n",
       "   77,\n",
       "   64,\n",
       "   63,\n",
       "   66,\n",
       "   148,\n",
       "   67,\n",
       "   78,\n",
       "   124,\n",
       "   63,\n",
       "   20,\n",
       "   125,\n",
       "   99,\n",
       "   151,\n",
       "   1,\n",
       "   57,\n",
       "   149,\n",
       "   165,\n",
       "   73,\n",
       "   27,\n",
       "   80,\n",
       "   90,\n",
       "   139,\n",
       "   65,\n",
       "   145,\n",
       "   149,\n",
       "   100,\n",
       "   16,\n",
       "   148,\n",
       "   63,\n",
       "   145,\n",
       "   71,\n",
       "   77,\n",
       "   10,\n",
       "   176,\n",
       "   125,\n",
       "   79,\n",
       "   165,\n",
       "   28,\n",
       "   99,\n",
       "   149,\n",
       "   90,\n",
       "   165,\n",
       "   74,\n",
       "   168,\n",
       "   99,\n",
       "   165,\n",
       "   72],\n",
       "  [36,\n",
       "   13,\n",
       "   57,\n",
       "   55,\n",
       "   9,\n",
       "   57,\n",
       "   31,\n",
       "   22,\n",
       "   45,\n",
       "   41,\n",
       "   51,\n",
       "   5,\n",
       "   57,\n",
       "   24,\n",
       "   44,\n",
       "   5,\n",
       "   19,\n",
       "   48,\n",
       "   29,\n",
       "   16,\n",
       "   24,\n",
       "   12,\n",
       "   21,\n",
       "   18,\n",
       "   44,\n",
       "   51,\n",
       "   29,\n",
       "   12,\n",
       "   54,\n",
       "   8,\n",
       "   1,\n",
       "   35,\n",
       "   35,\n",
       "   7,\n",
       "   37,\n",
       "   12,\n",
       "   4,\n",
       "   58,\n",
       "   50,\n",
       "   17,\n",
       "   30,\n",
       "   51,\n",
       "   29,\n",
       "   56,\n",
       "   54,\n",
       "   41,\n",
       "   44,\n",
       "   32,\n",
       "   38,\n",
       "   31,\n",
       "   26,\n",
       "   41,\n",
       "   57,\n",
       "   18,\n",
       "   22,\n",
       "   7,\n",
       "   9,\n",
       "   38,\n",
       "   24],\n",
       "  [],\n",
       "  [7,\n",
       "   4,\n",
       "   48,\n",
       "   47,\n",
       "   44,\n",
       "   4,\n",
       "   18,\n",
       "   25,\n",
       "   18,\n",
       "   47,\n",
       "   46,\n",
       "   18,\n",
       "   4,\n",
       "   47,\n",
       "   39,\n",
       "   44,\n",
       "   7,\n",
       "   40,\n",
       "   4,\n",
       "   33,\n",
       "   18,\n",
       "   41,\n",
       "   1,\n",
       "   45,\n",
       "   33,\n",
       "   39,\n",
       "   21,\n",
       "   33,\n",
       "   39,\n",
       "   47,\n",
       "   4,\n",
       "   48,\n",
       "   50,\n",
       "   46,\n",
       "   12,\n",
       "   25,\n",
       "   41,\n",
       "   33,\n",
       "   23,\n",
       "   49,\n",
       "   45,\n",
       "   43,\n",
       "   48,\n",
       "   40,\n",
       "   4,\n",
       "   20,\n",
       "   41,\n",
       "   18,\n",
       "   52,\n",
       "   40,\n",
       "   28,\n",
       "   4,\n",
       "   48,\n",
       "   46],\n",
       "  [3, 2, 2, 2],\n",
       "  [15,\n",
       "   59,\n",
       "   84,\n",
       "   66,\n",
       "   7,\n",
       "   11,\n",
       "   26,\n",
       "   45,\n",
       "   62,\n",
       "   83,\n",
       "   57,\n",
       "   74,\n",
       "   84,\n",
       "   40,\n",
       "   23,\n",
       "   80,\n",
       "   41,\n",
       "   70,\n",
       "   58,\n",
       "   64,\n",
       "   57,\n",
       "   63,\n",
       "   23,\n",
       "   42,\n",
       "   74,\n",
       "   51,\n",
       "   2,\n",
       "   51,\n",
       "   11,\n",
       "   26,\n",
       "   7,\n",
       "   34,\n",
       "   43,\n",
       "   50,\n",
       "   49,\n",
       "   84,\n",
       "   74,\n",
       "   11,\n",
       "   26,\n",
       "   57,\n",
       "   13,\n",
       "   33,\n",
       "   84,\n",
       "   13,\n",
       "   26,\n",
       "   58,\n",
       "   38,\n",
       "   56,\n",
       "   55,\n",
       "   55,\n",
       "   5,\n",
       "   7,\n",
       "   23,\n",
       "   75,\n",
       "   83,\n",
       "   58,\n",
       "   7,\n",
       "   82,\n",
       "   57,\n",
       "   60,\n",
       "   48,\n",
       "   51,\n",
       "   22,\n",
       "   58,\n",
       "   64,\n",
       "   57,\n",
       "   63,\n",
       "   69,\n",
       "   54,\n",
       "   56,\n",
       "   56,\n",
       "   10,\n",
       "   11,\n",
       "   26,\n",
       "   7,\n",
       "   62,\n",
       "   75,\n",
       "   22,\n",
       "   57,\n",
       "   51,\n",
       "   18,\n",
       "   75,\n",
       "   48,\n",
       "   53,\n",
       "   69],\n",
       "  [1, 1],\n",
       "  [24,\n",
       "   4,\n",
       "   21,\n",
       "   3,\n",
       "   2,\n",
       "   1,\n",
       "   11,\n",
       "   18,\n",
       "   18,\n",
       "   2,\n",
       "   4,\n",
       "   24,\n",
       "   10,\n",
       "   7,\n",
       "   16,\n",
       "   20,\n",
       "   17,\n",
       "   9,\n",
       "   14,\n",
       "   22,\n",
       "   16,\n",
       "   18,\n",
       "   18,\n",
       "   1,\n",
       "   14,\n",
       "   25],\n",
       "  [],\n",
       "  [26,\n",
       "   24,\n",
       "   11,\n",
       "   1,\n",
       "   8,\n",
       "   7,\n",
       "   37,\n",
       "   10,\n",
       "   16,\n",
       "   34,\n",
       "   8,\n",
       "   32,\n",
       "   39,\n",
       "   1,\n",
       "   18,\n",
       "   5,\n",
       "   38,\n",
       "   1,\n",
       "   1,\n",
       "   15,\n",
       "   41,\n",
       "   14,\n",
       "   32,\n",
       "   46,\n",
       "   1,\n",
       "   34,\n",
       "   45,\n",
       "   34,\n",
       "   11,\n",
       "   34,\n",
       "   43,\n",
       "   11,\n",
       "   45,\n",
       "   13,\n",
       "   34,\n",
       "   4,\n",
       "   40,\n",
       "   9,\n",
       "   7,\n",
       "   11,\n",
       "   40,\n",
       "   1,\n",
       "   22,\n",
       "   7,\n",
       "   37,\n",
       "   1,\n",
       "   13],\n",
       "  [1, 1, 2],\n",
       "  [12, 6, 8, 8, 1, 17, 9, 3, 14, 11, 3, 12, 6, 19, 7, 10, 5, 14, 1, 7, 20]],\n",
       " 'padded_context': array([[ 49,   7,  24, ...,   0,   0,   0],\n",
       "        [165,  90,  99, ...,  99, 165,  72],\n",
       "        [ 36,  13,  57, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [ 26,  24,  11, ...,   0,   0,   0],\n",
       "        [  1,   1,   2, ...,   0,   0,   0],\n",
       "        [ 12,   6,   8, ...,   0,   0,   0]], dtype=int32),\n",
       " 'targets': array([[  7,  24,  11, ...,   0,   0,   0],\n",
       "        [ 90,  99,  94, ..., 165,  72,   0],\n",
       "        [ 13,  57,  55, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [ 24,  11,   1, ...,   0,   0,   0],\n",
       "        [  1,   2,   0, ...,   0,   0,   0],\n",
       "        [  6,   8,   8, ...,   0,   0,   0]])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = []\n",
    "targ = []\n",
    "for ctx in context:\n",
    "    for cx in ctx:\n",
    "        cont.append(cx)\n",
    "        targ.append(np.argmax(pred[cx]))\n",
    "df = pd.DataFrame({'Word':cont, 'Next Predicted Word':targ})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "8hb5s",
   "launcher_item_id": "5NrJ6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
